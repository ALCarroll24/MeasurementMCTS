{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import timeit\n",
    "from typing import List, Tuple\n",
    "\n",
    "# MeasurementMCTS Python Package code imports\n",
    "sys.path.append(\"..\")  # Adds higher directory to python modules path.\n",
    "from main import MeasurementControlEnvironment\n",
    "from utils import rotate_about_point\n",
    "from state_evaluation.reinforcement_learning import MCTSRLWrapper\n",
    "from mcts.mcts import mcts_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy Measurement Control Initialized\n",
      "Using device: cuda\n",
      "Model loaded\n",
      "Actions in action space: 25\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n",
      "Inference shape: (26,)\n",
      "Action probs shape: (25,)\n",
      "Value shape: ()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Run the MCTS search\u001b[39;00m\n\u001b[1;32m     10\u001b[0m start \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[0;32m---> 11\u001b[0m best_action \u001b[38;5;241m=\u001b[39m \u001b[43mmcts_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# env.draw_state(init_state)\u001b[39;00m\n",
      "File \u001b[0;32m~/MeasurementMCTS/state_evaluation/../mcts/mcts.py:258\u001b[0m, in \u001b[0;36mmcts_search\u001b[0;34m(env, eval, starting_state, learning_iterations)\u001b[0m\n\u001b[1;32m    255\u001b[0m     leaf\u001b[38;5;241m.\u001b[39mexpand(child_priors) \u001b[38;5;66;03m# Expand the leaf node with the child priors\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     leaf\u001b[38;5;241m.\u001b[39mbackup(value_estimate) \u001b[38;5;66;03m# Backup the value estimate up the tree to the root node\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Optimize the model using replay memory which we just added one transition to\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(root\u001b[38;5;241m.\u001b[39mchild_number_visits), root\n",
      "File \u001b[0;32m~/MeasurementMCTS/state_evaluation/../state_evaluation/reinforcement_learning.py:241\u001b[0m, in \u001b[0;36mMCTSRLWrapper.optimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Backpropagate the loss (calculate gradients for each parameter)\u001b[39;00m\n\u001b[1;32m    244\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/MeasurementMCTS/state_evaluation/../state_evaluation/reinforcement_learning.py:216\u001b[0m, in \u001b[0;36mMCTSRLWrapper.loss\u001b[0;34m(self, transitions)\u001b[0m\n\u001b[1;32m    213\u001b[0m y_targets \u001b[38;5;241m=\u001b[39m reward_transitions \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m max_next_q_value \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m~\u001b[39mdone_transitions \u001b[38;5;66;03m# If done, the target is just the reward\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Get the current q_values, pick the values that are from the actions taken and then squeeze the tensor (remove the extra dimension)\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_transitions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_transitions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Calculate the loss (difference between the target and the current q_value prediction)\u001b[39;00m\n\u001b[1;32m    219\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(y_targets, q_values)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = MeasurementControlEnvironment(notebook=True, no_flask_server=True)\n",
    "init_state = env.reset()\n",
    "\n",
    "# Create the state evaluation neural network object\n",
    "eval = MCTSRLWrapper(env, 'new', env.action_space.shape[0])\n",
    "print(f'Actions in action space: {env.action_space.shape[0]}')\n",
    "\n",
    "# Run the MCTS search\n",
    "start = timeit.default_timer()\n",
    "best_action = mcts_search(env, eval, init_state, 1000)\n",
    "\n",
    "# env.draw_state(init_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
