{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# MCTS code imports\n",
    "sys.path.append(\"..\")  # Adds higher directory to python modules path.\n",
    "from main import ToyMeasurementControl\n",
    "from utils import rotate_about_point\n",
    "\n",
    "class PolicyValueNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural net combining policy and value network (state -> (policy, value))\n",
    "    params:\n",
    "        state_dims: Number of dimensions in the state space\n",
    "        action_dims: Number of dimensions in the action space\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dims, action_dims):\n",
    "        super(PolicyValueNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dims, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, action_dims+1) # +1 for the value output\n",
    "\n",
    "    # Called with either one element to determine next action, or a transitions\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "\n",
    "# Named tuple for transitions\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    Replay memory for storing transitions\n",
    "    Creates a deque with a maximum length of capacity. Transitions are stored as named tuples.\n",
    "    \n",
    "    methods:\n",
    "        push(*args): Save a transition\n",
    "        sample(batch_size): Sample a batch of transitions\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class MCTSRLWrapper:\n",
    "    def __init__(self, q_network: PolicyValueNetwork, target_q_network: PolicyValueNetwork,\n",
    "                 replay_memory: ReplayMemory, gamma: float=0.99, batch_size: int=64, \n",
    "                 lr: float=0.001, tau: float=0.005):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Using device:\", self.device)\n",
    "        \n",
    "        self.q_network = q_network.to(self.device) # Q Network which is changed only after full batch is processed\n",
    "        self.target_q_network = target_q_network.to(self.device) # Q network which is updated during batch training\n",
    "        self.replay_memory = replay_memory # Replay memory for storing transitions\n",
    "        self.gamma = gamma # Discount factor\n",
    "        self.batch_size = batch_size # Number of transitions to sample for training\n",
    "        self.tau = tau # Soft update parameter for target network (target = tau * q_network + (1 - tau) * target_q_network)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr, amsgrad=True) # Adam optimizer for training the Q network\n",
    "        \n",
    "    def loss(self, transitions):\n",
    "        \"\"\"\n",
    "        Calculate the loss for a batch of transitions\n",
    "        \"\"\"\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        transitions = Transition(*zip(*transitions))\n",
    "        \n",
    "        # Pull out the components of the batch and concatinate them (concatenate scalar lists and stack tensor lists)\n",
    "        state_transitions = torch.stack(transitions.state)\n",
    "        action_transitions = torch.cat(transitions.action)\n",
    "        next_state_transitions = torch.stack(transitions.next_state)\n",
    "        reward_transitions = torch.cat(transitions.reward)\n",
    "        done_transitions = torch.cat(transitions.done)\n",
    "        \n",
    "        # Get max Q target values of next state from target network (max_a' Q_target(s', a'))\n",
    "        max_next_q_value = self.target_q_network(next_state_transitions).max(dim=1).values\n",
    "        \n",
    "        # Calculate the target (r + Î³ * max_a' Q_target(s', a')), what we want the Q network to predict\n",
    "        y_targets = reward_transitions + self.gamma * max_next_q_value * ~done_transitions # If done, the target is just the reward\n",
    "        \n",
    "        # Get the current q_values, pick the values that are from the actions taken and then squeeze the tensor (remove the extra dimension)\n",
    "        q_values = self.q_network(state_transitions).gather(1, action_transitions).squeeze()\n",
    "\n",
    "        # Calculate the loss (difference between the target and the current q_value prediction)\n",
    "        loss = F.mse_loss(y_targets, q_values)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        # Check if there are enough transitions in the replay memory to optimize\n",
    "        if len(self.replay_memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample a batch of transitions\n",
    "        transitions = self.replay_memory.sample(self.batch_size)\n",
    "        \n",
    "        # Set the network to training mode\n",
    "        self.q_network.train()\n",
    "        \n",
    "        # Zero the gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = self.loss(transitions)\n",
    "        \n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform a step of optimization\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Soft update of the target network\n",
    "        for target_param, param in zip(self.target_q_network.parameters(), self.q_network.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create state image from car position, OOI corners, car width, car_length and obstacles\n",
    "def get_image_based_state(env: ToyMeasurementControl, state: tuple, width_pixels=200, width_meters=50) -> tuple():\n",
    "    # Get car collision length and width\n",
    "    car_width, car_length = env.car.width, env.car.length\n",
    "    \n",
    "    # Get obstacle means and radii\n",
    "    obstacle_means, obstacle_radii = env.eval_kd_tree.get_obstacle_points(), env.eval_kd_tree.get_obstacle_radii()\n",
    "\n",
    "    # Pull out the state components\n",
    "    car_state, corner_means, corner_covariance, horizon = state\n",
    "    corner_means = corner_means.reshape(-1, 2) # Reshape to 2D array where each row is a corner point\n",
    "    \n",
    "    # Get normalized point covariances\n",
    "    pt_traces = env.get_normalized_cov_pt_traces(state)\n",
    "    \n",
    "    # Since image is body frame representation of car, obstacles and OOIs. The neural net only needs [vx, delta, delta_dot] as input\n",
    "    # These are the components of the state which will determine how actions effect the car state, the rest of the state is used to generate the image\n",
    "    nn_car_state = car_state[[2, 4, 5]]\n",
    "    \n",
    "    # Make the image\n",
    "    image = np.zeros((width_pixels, width_pixels), dtype=np.float32)\n",
    "    \n",
    "    # Calculate the scaling factor from meters to pixels\n",
    "    scale = width_pixels / width_meters\n",
    "    \n",
    "    # Rotate the obstacle and corner points to the car's yaw angle\n",
    "    car_pos, car_yaw = car_state[:2], car_state[3]\n",
    "    rotated_corners = rotate_about_point(corner_means, np.pi/2-car_yaw, car_pos) # Negative to rotate into a coordinate system where the car is facing up\n",
    "    rotated_obstacles = rotate_about_point(obstacle_means, np.pi/2-car_yaw, car_pos)\n",
    "    \n",
    "    # Subtract the car's position from the rotated points to get the points relative to the car\n",
    "    rotated_corners -= car_state[:2]\n",
    "    rotated_obstacles -= car_state[:2]\n",
    "    \n",
    "    # Find which points are within the image bounds\n",
    "    in_bounds_corners = (-width_meters/2 <= rotated_corners[:, 0]) & (rotated_corners[:, 0] <= width_meters/2) & \\\n",
    "                        (-width_meters/2 <= rotated_corners[:, 1]) & (rotated_corners[:, 1] <= width_meters/2)\n",
    "\n",
    "    in_bounds_obstacles = (-width_meters/2 <= rotated_obstacles[:, 0]) & (rotated_obstacles[:, 0] <= width_meters/2) & \\\n",
    "                          (-width_meters/2 <= rotated_obstacles[:, 1]) & (rotated_obstacles[:, 1] <= width_meters/2)\n",
    "\n",
    "    # Convert the car frame in bounds points to pixel coordinates\n",
    "    in_bounds_corner_pixels = (rotated_corners[in_bounds_corners] * scale + width_pixels / 2).astype(int)\n",
    "    in_bounds_obstacle_pixels = (rotated_obstacles[in_bounds_obstacles] * scale + width_pixels / 2).astype(int)\n",
    "    in_bounds_obstacle_radii_pixels = (obstacle_radii[in_bounds_obstacles] * scale).astype(int)\n",
    "    \n",
    "    # First place obstacles so that rewards and car overlay them\n",
    "    for i, point in enumerate(in_bounds_obstacle_pixels):\n",
    "        x_pixel, y_pixel = point\n",
    "        radius_pixel = in_bounds_obstacle_radii_pixels[i]\n",
    "        x, y = np.ogrid[-x_pixel:width_pixels-x_pixel, -y_pixel:width_pixels-y_pixel]\n",
    "        mask = x*x + y*y <= radius_pixel*radius_pixel\n",
    "        image[mask] = -1.0\n",
    "        \n",
    "    # Place the car (draw a rectangle at the center given length and width), car is always facing up (positive x axis)\n",
    "    car_width_pixels = int(car_width * scale)\n",
    "    car_length_pixels = int(car_length * scale)\n",
    "    car_max_x_index = car_width_pixels + width_pixels // 2\n",
    "    car_max_y_index = car_length_pixels + width_pixels // 2\n",
    "    image[-car_max_x_index:car_max_x_index, -car_max_y_index:car_max_y_index] = -0.5\n",
    "    \n",
    "    # Place the corners\n",
    "    image[in_bounds_corner_pixels[:, 0], in_bounds_corner_pixels[:, 1]] = pt_traces[in_bounds_corners]\n",
    "        \n",
    "    # Return the neural net state and the image\n",
    "    return nn_car_state, image\n",
    "\n",
    "def get_nn_state(env: ToyMeasurementControl, state: tuple, device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert full state into image and then combine car state and flattened image into a single tensor\n",
    "    params:\n",
    "        env: ToyMeasurementControl environment\n",
    "        state: Full state tuple\n",
    "        device: Device to put the tensor on\n",
    "    \"\"\"\n",
    "    nn_car_state, image = get_image_based_state(env, state)\n",
    "    \n",
    "    nn_state = torch.cat((torch.tensor(nn_car_state, dtype=torch.float32, device=device),\n",
    "                          torch.tensor(image.flatten(), dtype=torch.float32, device=device)))\n",
    "    \n",
    "    return nn_state\n",
    "\n",
    "def plot_state_image(image, title):\n",
    "    plt.imshow(image.T, cmap='gray', origin='lower')\n",
    "    plt.colorbar(label='Value')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy Measurement Control Initialized\n",
      "Using device: cuda\n",
      "Q values after .gather(1, action_transitions): tensor([ 0.0408,  0.0436,  0.0913,  0.0691, -0.0834,  0.0582,  0.0296,  0.0599,\n",
      "         0.0638, -0.0413], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "Shape of y_targets: torch.Size([10])\n",
      "Shape of q_values: torch.Size([10])\n",
      "Optimization complete\n"
     ]
    }
   ],
   "source": [
    "# Make a toy measurement control object (environment for MCTS)\n",
    "tmc = ToyMeasurementControl(no_flask_server=True, enable_ui=False)\n",
    "\n",
    "# Get the initial state sizes using the NN state space\n",
    "nn_car_state, image = get_image_based_state(tmc, tmc.get_state())\n",
    "\n",
    "# Get the observation (NN state space) length\n",
    "nn_state_dims = nn_car_state.size + image.size\n",
    "num_actions = len(tmc.all_actions) # Number of actions (rows)\n",
    "\n",
    "# Create the Q network and target Q network\n",
    "q_network = PolicyValueNetwork(nn_state_dims, num_actions)\n",
    "target_q_network = PolicyValueNetwork(nn_state_dims, num_actions)\n",
    "\n",
    "# Create the replay memory\n",
    "replay_memory = ReplayMemory(10000)\n",
    "\n",
    "# Create the MCTSRL wrapper\n",
    "mcts_rl = MCTSRLWrapper(q_network, target_q_network, replay_memory, gamma=0.99, batch_size=10, lr=0.001, tau=0.005)\n",
    "\n",
    "# Create some random transitions to test the loss function\n",
    "state = tmc.get_state()\n",
    "for i in range(10):\n",
    "    # Get the image based state\n",
    "    nn_car_state = get_nn_state(tmc, state, mcts_rl.device)\n",
    "    \n",
    "    # Get the action\n",
    "    action = torch.tensor([[random.randint(0, num_actions-1)]], dtype=torch.int64, device=mcts_rl.device)\n",
    "    \n",
    "    # Take the action\n",
    "    next_state, reward, done = tmc.step(state, action.item())\n",
    "    done = bool(done) # Convert to python boolean from np.bool_ (removes warning)\n",
    "    \n",
    "    # Get the next image based state\n",
    "    next_nn_car_state = get_nn_state(tmc, next_state, mcts_rl.device)\n",
    "    \n",
    "    # Save the transition\n",
    "    replay_memory.push(nn_car_state, action, next_nn_car_state,\n",
    "                       torch.tensor([reward], dtype=torch.float32, device=mcts_rl.device),\n",
    "                       torch.tensor([done],   dtype=torch.uint8,   device=mcts_rl.device))\n",
    "    \n",
    "    # Update the current state\n",
    "    state = next_state\n",
    "    \n",
    "# Optimize the model\n",
    "mcts_rl.optimize_model()\n",
    "print(\"Optimization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN State shape: torch.Size([40003])\n",
      "NN State shape: torch.Size([40003])\n",
      "tensor([ 0.0074, -0.0027,  0.0533, -0.0953,  0.0834,  0.0507,  0.0026,  0.0062,\n",
      "         0.0528, -0.0193, -0.0772,  0.0472, -0.0458,  0.0681,  0.0185, -0.0296,\n",
      "         0.0027, -0.0566,  0.0678, -0.0526, -0.0072,  0.0164,  0.0632, -0.0376,\n",
      "         0.0647,  0.0160], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Inference time: 0.0024198060000344412\n"
     ]
    }
   ],
   "source": [
    "# grab a random state\n",
    "state = tmc.get_state()\n",
    "nn_state = get_nn_state(tmc, state, mcts_rl.device)\n",
    "print(f'NN State shape: {nn_state.shape}')\n",
    "\n",
    "# Try inference\n",
    "start_time = timeit.default_timer()\n",
    "print(q_network(nn_state))\n",
    "print(f'Inference time: {timeit.default_timer() - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple of tensors: (tensor([1, 2, 3], device='cuda:0'), tensor([4, 5, 6], device='cuda:0'))\n",
      "Concatenated tensors: torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3, 5], device='cuda:0')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple_of_tensors = (torch.tensor([1, 2, 3], device=mcts_rl.device), torch.tensor([4, 5, 6], device=mcts_rl.device))\n",
    "print(f'Tuple of tensors: {tuple_of_tensors}')\n",
    "print(f'Concatenated tensors: {torch.stack(tuple_of_tensors).shape}')\n",
    "tensor_stack = torch.stack(tuple_of_tensors)\n",
    "tensor_stack.gather(1, torch.tensor([[2], [1]], device=mcts_rl.device)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple of tensors: (tensor([1], device='cuda:0'), tensor([4], device='cuda:0'))\n",
      "Concatenated tensors: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "tuple_of_tensors = (torch.tensor([1], device=mcts_rl.device), torch.tensor([4], device=mcts_rl.device))\n",
    "print(f'Tuple of tensors: {tuple_of_tensors}')\n",
    "print(f'Concatenated tensors: {torch.cat(tuple_of_tensors).shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
